import requests
import os
import json
from bs4 import BeautifulSoup, Comment
import dns.resolver  #lib_name: dnspython
import socket

def load_config():
    json_file_name = os.path.join("config", "settings.json")
    absolute_path = os.path.abspath(json_file_name)

    try:
        with open(absolute_path, 'r', encoding='utf-8') as json_file:
            return json.load(json_file)
    except FileNotFoundError:
        print(f"[ERROR] JSON File Not Found : {absolute_path}")
    except json.JSONDecodeError:
        print(f"[ERROR JSON File is invalid : {absolute_path}]")
    return None

def fetch_response(base_url):
    try :
        config = load_config()
        timeout = config["http_settings"]["timeout"]
        response = requests.get(base_url, timeout=timeout)
        return response
    except requests.RequestException as e :
        print(f"[ERROR] Failed to fetch URL {base_url}: {e}")
        return None 

def analyze_http_headers(response, log_file):
    print("\n[INFO] HTTP Headers:")
    with open(log_file, 'a') as log:
        log.write("\n[INFO] HTTP Headers:\n")
        for header, value in response.headers.items():
            print(f"{header} : {value}")
            log.write(f"{header}: {value}\n")

def analyze_security_headers(response, log_file):
    security_headers = {
        "Content-Security-Policy": "CSP (Content-Security-Policy)",
        "Strict-Transport-Security": "HSTS (Strict-Transport-Security)",
        "X-Frame-Options": "X-Frame-Options",
        "X-Content-Type-Options": "X-Content-Type-Options",
        "X-XSS-Protection": "X-XSS-Protection",
        "Permissions-Policy": "Permissions-Policy",
    }

    print("\n[INFO] Security Headers:")
    with open(log_file, 'a') as log:
        log.write("\n[INFO] Security Headers:")
        for header, description in security_headers.items():
            value = response.headers.get(header, None)
            if value:
                print(f"- {header} : {value}")
                log.write(f"- {header}: {value}\n")
            else:
                print(f"[WARNING] {header} ({description}) is missing.")
                log.write(f"[WARNING] {header} ({description}) is missing.\n")
    
def analyze_cors_headers(response, log_file):
    cors_headers = {
        "Access-Control-Allow-Origin": response.headers.get("Access-Control-Allow-Origin", "Not Found"),
        "Access-Control-Allow-Credentials": response.headers.get("Access-Control-Allow-Credentials", "Not Found"),
        "Access-Control-Allow-Methods": response.headers.get("Access-Control-Allow-Methods", "Not Found"),
        "Access-Control-Allow-Headers": response.headers.get("Access-Control-Allow-Headers", "Not Found"),
        "Access-Control-Expose-Headers": response.headers.get("Access-Control-Expose-Headers", "Not Found"),
        "Access-Control-Max-Age": response.headers.get("Access-Control-Max-Age", "Not Found"),
    }

    print("\n[INFO] CORS Headers:")
    with open(log_file, 'a') as log:
        log.write("\n[INFO] CORS Headers:\n")
        for header, value in cors_headers.items():
            print(f"- {header} : {value}")
            log.write(f"- {header}: {value}\n")

        origin = cors_headers["Access-Control-Allow-Origin"]
        credentials = cors_headers["Access-Control-Allow-Credentials"]

        if origin == "*" and credentials == "true":
            print("[CRITICAL] Combination of '*' and 'true' in CORS headers exposes sensitive data.")
            log.write("[CRITICAL] Combination of '*' and 'true' in CORS headers exposes sensitive data.\n")   

def scan_subdomains(config, log_file):
    url_template = config.get("url")
    subdomains = config.get("subdomains", [])
    
    if not url_template or "{sub}" not in url_template:
        print("[ERROR] No subdomains defined in settings.json.")
        with open(log_file, 'a') as log:
            log.write("[ERROR] No subdomains defined in settings.json.\n")
        return 

    discovered = []

    print("\n[INFO] Scanning subdomains...")
    with open(log_file, 'a') as log:
        log.write("\n[INFO] Scanning subdomains...\n")

        for sub in subdomains:
            target_url = url_template.replace("{sub}", sub)
            try:
                response = requests.get(target_url, timeout=5)
                if response.status_code == 200:
                    print(f"[FOUND] {target_url}")
                    log.write(f"[FOUND] {target_url}\n")
                    discovered.append(target_url)
            except requests.RequestException:
                pass

    if not discovered:
        print("[INFO] No subdomains discovered.")
        with open(log_file, 'a') as log:
            log.write("[INFO] No subdomains discovered.\n")

def discover_hidden_files_from_config(config, log_file):
    url = config.get("base_url", "").strip("/")
    hidden_files = config.get("hidden_files", [])

    if not hidden_files:
        print("[ERROR] No hidden files or directories defined in settings.json.")
        with open(log_file, 'a') as log:
            log.write("[ERROR] No hidden files or directories defined in settings.json.")
        return
    
    print("\n[INFO] Discovering hidden files and directories...")
    with open(log_file, 'a') as log:
        log.write("\n[INFO] Discovering hidden files and directories...\n")

        for file in hidden_files:
            target_url = f"{url}/{file}"
            try:
                response = requests.get(target_url, timeout=5)
                if response.status_code == 200:
                    print(f"[FOUND] {target_url}")
                    log.write(f"[FOUND] {target_url}\n")
            except requests.RequestException:
                pass

def find_api_endpoints(config, log_file):
    base_url = config.get("base_url")
    api_endpoints = config.get("api_endpoints", [])

    if not api_endpoints:
        print("[INFO] No API endpoints defined in settings.json.")
        with open(log_file, 'a') as log:
            log.write("[INFO] No API endpoints defined in settings.json.\n")
        return
    
    print("\n[INFO] Discovering API endpoints...")
    with open(log_file, 'a') as log:
        log.write("\n[INFO] Discovering API endpoints...\n")

        for endpoint in api_endpoints:
            target_url = f"{base_url}{endpoint}"
            try:
                response = requests.get(target_url, timeout=5)
                if response.status_code == 200:
                    print(f"[FOUND] {target_url}")
                    log.write(f"[FOUND] {target_url}\n")
                else:
                    print(f"[INFO] {target_url} returned status code {response.status_code}")
                    log.write(f"[INFO] {target_url} returned status code {response.status_code}\n")
            except requests.RequestException as e:
                print(f"[ERROR] Failed to connect to {target_url}: {e}")
                with open(log_file, 'a') as log:
                    log.write(f"[ERROR] Failed to connect to {target_url}: {e}\n")

def analyze_html_metadata(config, log_file):
    base_url = config.get("base_url")

    print("\n[INFO] Anlyzing HTML metadata...")
    with open(log_file, 'a') as log:
        log.write("\n[INFO] Anlyzing HTML metadata...\n")

        try:
            response = requests.get(base_url, timeout=5)
            soup = BeautifulSoup(response.text, 'html.parser')

            for meta in soup.find_all('meta'):
                print(f"[META] {meta}")
                log.write(f"[META] {meta}\n")

            comments = soup.find_all(string=lambda text: isinstance(text, Comment))
            for comment in comments:
                print(f"[COMMENT] {comment}")
                log.write(f"[COMMENT] {comment}\n")

        except requests.RequestException as e:
            print(f"[ERROR] Failed to analyze metadata for {base_url}: {e}")
            with open(log_file, 'a') as log:
                log.write(f"[ERROR] Failed to analyze metadata for {base_url}: {e}\n")

def collect_dns_info(config, log_file):
    domain = config.get("base_url").replace("http://","").replace("https://","").split('/')[0]
    dns_records = config.get("dns_records", ["A", "MX", "NS"])

    print("\n[INFO] Collecting DNS information...")
    with open(log_file, 'a') as log:
        log.write("\n[INFO] Collecting DNS information...\n")

        for record_type in dns_records:
            try:
                answers = dns.resolver.resolve(domain, record_type)
                for rdata in answers:
                    print(f"{record_type}: {rdata}")
                    log.write(f"{record_type}: {rdata}\n")
            except Exception as e:
                print(f"[ERROR]Failed to resolve {record_type} record: {e}")
                with open(log_file, 'a') as log:
                    log.write(f"[ERROR] Failed to resolve {record_type} record: {e}\n")

def port_scan_with_banner(config, log_file):
    base_url = config.get("base_url").replace("http://", "").replace("https://","").split('/')[0]
    ports = config.get("ports_to_scan", [21,22,23,80,443,3306,8080])

    print("\n[INFO] Performing port scan...")
    with open(log_file, 'a') as log:
        log.write("\n[INFO] Performing Port Scan with Banner Collection...\n")

        for port in ports:
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.settimeout(1)
                    result = s.connect_ex((base_url, port))
                    if result == 0:
                        print(f"[OPEN] Port {port} is open.")
                        log.write(f"[OPEN] port {port} is open.\n")

                        try:
                            s.sendall(b'HEAD / HTTP/1.0/r/n/r/n')
                            banner = s.recv(1024).decode("utf-8", errors="ignore").strip()
                            print(f"    [BANNER] {banner}")
                            log.write(f"    [BANNER] {banner}\n")
                        except Exception as banner_err:
                            print(f"    [ERROR] Failed to fetch banner for port {port}: {banner_err}")
                            log.write(f"    [ERROR] Failed to fetch banner for port {port}: {banner_err}\n")
                    else:
                        print(f"[CLOSED] Port {port} is closed")
            except Exception as e:
                print(f"[ERROR] Failed to scan port {port}: {e}")
                with open(log_file, 'a') as log:
                    log.write(f"[ERROR] Failed to scan port {port}: {e}\n")
    

def run_recon(log_file):
    config = load_config()
    if not config:
        return
    
    base_url = config.get("base_url")
    if not base_url:
        print("[ERROR] Base URL is not defined in the configuration file.")
        return
    
    response = fetch_response(base_url)
    if not response:
        return
    
    print("==========================================")
    print("=============== Recon start ==============")
    print("==========================================")
    analyze_http_headers(response, log_file)
    analyze_security_headers(response, log_file)
    analyze_cors_headers(response, log_file)
    scan_subdomains(config, log_file)
    discover_hidden_files_from_config(config, log_file)
    find_api_endpoints(config, log_file)
    analyze_html_metadata(config, log_file)
    collect_dns_info(config, log_file)
    port_scan_with_banner(config, log_file)

    



